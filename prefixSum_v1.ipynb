{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prefix Sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import pycuda.autoinit\n",
    "import pycuda.driver\n",
    "import pycuda.compiler\n",
    "\n",
    "import math\n",
    "\n",
    "import timeit \n",
    "\n",
    "from collections import OrderedDict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## generate data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 5\n",
    "input_data = np.asarray([ 2**i for i in range(1, n+1) ]).astype(np.uint32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CPU prefix sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getPrefixSum_cpu(input_data, n):\n",
    "    prefix_sum_cpu = np.zeros_like(input_data)\n",
    "\n",
    "    for i in range(1,n):\n",
    "        prefix_sum_cpu[i] = prefix_sum_cpu[i-1] + input_data[i-1]\n",
    "    return prefix_sum_cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "prefix_sum_cpu = getPrefixSum_cpu(input_data, n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getPrefixSum_gpu(input_data, n):\n",
    "    # DEFINE block GPU\n",
    "    source_module = pycuda.compiler.SourceModule \\\n",
    "    (\n",
    "            \"\"\"\n",
    "            __global__ void prefix_sum_up_sweep( unsigned int* d_prefix_sum, int n, int d )\n",
    "            {\n",
    "                int global_index_1d = ( blockIdx.x * blockDim.x ) + threadIdx.x;\n",
    "                int k               = global_index_1d * ( 2 << d );\n",
    "        \n",
    "                int left_index;\n",
    "                int right_index;\n",
    "        \n",
    "                if ( d == 0 )\n",
    "                {\n",
    "                    left_index  = k;\n",
    "                    right_index = k + 1;\n",
    "                }\n",
    "                else\n",
    "                {\n",
    "                    left_index  = k + ( 2 << ( d - 1 ) ) - 1;\n",
    "                    right_index = k + ( 2 << d )         - 1;\n",
    "                }\n",
    "        \n",
    "                if ( right_index < n )\n",
    "                {\n",
    "                    d_prefix_sum[ right_index ] = d_prefix_sum[ left_index ] + d_prefix_sum[ right_index ];\n",
    "                }\n",
    "            }\n",
    "        \n",
    "            __global__ void prefix_sum_down_sweep( unsigned int* d_prefix_sum, int n, int d )\n",
    "            {\n",
    "                int global_index_1d = ( blockIdx.x * blockDim.x ) + threadIdx.x;\n",
    "                int k               = global_index_1d * ( 2 << d );\n",
    "        \n",
    "                int left_index;\n",
    "                int right_index;\n",
    "        \n",
    "                if ( d == 0 )\n",
    "                {\n",
    "                    left_index  = k;\n",
    "                    right_index = k + 1;\n",
    "                }\n",
    "                else\n",
    "                {\n",
    "                    left_index  = k + ( 2 << ( d - 1 ) ) - 1;\n",
    "                    right_index = k + ( 2 << d )         - 1;\n",
    "                }\n",
    "        \n",
    "                if ( right_index < n )\n",
    "                {\n",
    "                    unsigned int temp           = d_prefix_sum[ right_index ];\n",
    "                    d_prefix_sum[ right_index ] = d_prefix_sum[ left_index ] + d_prefix_sum[ right_index ];\n",
    "                    d_prefix_sum[ left_index ]  = temp;\n",
    "                }\n",
    "            }\n",
    "        \n",
    "            __global__ void blocked_prefix_sum_set_last_block_elements_to_zero( unsigned int* d_prefix_sums, int n, int block_size_num_elements )\n",
    "            {\n",
    "                int global_index_1d_left  = ( ( ( threadIdx.x * 2 ) + 1 ) * block_size_num_elements ) - 1;\n",
    "                int global_index_1d_right = ( ( ( threadIdx.x * 2 ) + 2 ) * block_size_num_elements ) - 1;\n",
    "        \n",
    "                if ( global_index_1d_left < n )\n",
    "                {\n",
    "                    d_prefix_sums[ global_index_1d_left ] = 0;\n",
    "                }\n",
    "        \n",
    "                if ( global_index_1d_right < n )\n",
    "                {\n",
    "                    d_prefix_sums[ global_index_1d_right ] = 0;\n",
    "                }\n",
    "            }\n",
    "        \n",
    "            __global__ void blocked_prefix_sum_down_sweep(\n",
    "                unsigned int* d_prefix_sum,\n",
    "                unsigned int* d_block_sums,\n",
    "                unsigned int* d_input_data_resized,\n",
    "                int n,\n",
    "                int d )\n",
    "            {\n",
    "                int global_index_1d = ( blockIdx.x * blockDim.x ) + threadIdx.x;\n",
    "                int k               = global_index_1d * ( 2 << d );\n",
    "        \n",
    "                int left_index;\n",
    "                int right_index;\n",
    "        \n",
    "                if ( d == 0 )\n",
    "                {\n",
    "                    left_index  = k;\n",
    "                    right_index = k + 1;\n",
    "                }\n",
    "                else\n",
    "                {\n",
    "                    left_index  = k + ( 2 << ( d - 1 ) ) - 1;\n",
    "                    right_index = k + ( 2 << d )         - 1;\n",
    "                }\n",
    "        \n",
    "                if ( right_index < n )\n",
    "                {\n",
    "                    unsigned int temp           = d_prefix_sum[ right_index ];\n",
    "                    d_prefix_sum[ right_index ] = d_prefix_sum[ left_index ] + d_prefix_sum[ right_index ];\n",
    "                    d_prefix_sum[ left_index ]  = temp;\n",
    "                }\n",
    "        \n",
    "                if ( d == 0 && threadIdx.x == blockDim.x - 1 )\n",
    "                {\n",
    "                    d_block_sums[ blockIdx.x ] = d_prefix_sum[ right_index ] + d_input_data_resized[ right_index ];\n",
    "                }\n",
    "            }\n",
    "        \n",
    "            __global__ void blocked_prefix_sum_add_block_sums( unsigned int* d_prefix_sums, unsigned int* d_block_sums, int n )\n",
    "            {\n",
    "                int global_index_1d = 2 * ( ( blockIdx.x * blockDim.x ) + threadIdx.x );\n",
    "        \n",
    "                if ( blockIdx.x > 0 && global_index_1d < n - 1 )\n",
    "                {\n",
    "                    unsigned int block_sum               = d_block_sums[ blockIdx.x ];\n",
    "                    d_prefix_sums[ global_index_1d ]     = d_prefix_sums[ global_index_1d ] + block_sum;\n",
    "                    d_prefix_sums[ global_index_1d + 1 ] = d_prefix_sums[ global_index_1d + 1 ] + block_sum;\n",
    "                }\n",
    "            }\n",
    "            \"\"\"\n",
    "    )\n",
    "\n",
    "    block_size_num_elements = 1024\n",
    "    block_size_num_threads = block_size_num_elements / 2\n",
    "\n",
    "    num_elements_to_pad = 0\n",
    "    if n % block_size_num_elements != 0:\n",
    "        num_elements_to_pad = block_size_num_elements - (n % block_size_num_elements)\n",
    "\n",
    "    input_data_resized_num_elements = n + num_elements_to_pad\n",
    "    input_data_resized_num_threads = input_data_resized_num_elements / 2\n",
    "\n",
    "    input_data_resized = np.zeros(input_data_resized_num_elements, dtype=input_data.dtype)\n",
    "    input_data_resized[0:n] = input_data\n",
    "    prefix_sum_gpu = np.zeros_like(input_data_resized)\n",
    "    block_sums_gpu = np.zeros(block_size_num_elements, dtype=input_data_resized.dtype)\n",
    "    input_data_resized_device = pycuda.driver.mem_alloc(input_data_resized.nbytes)\n",
    "    prefix_sum_device = pycuda.driver.mem_alloc(prefix_sum_gpu.nbytes)\n",
    "    block_sums_device = pycuda.driver.mem_alloc(block_sums_gpu.nbytes)\n",
    "\n",
    "    prefix_sum_down_sweep_function = source_module.get_function(\"prefix_sum_down_sweep\")\n",
    "    prefix_sum_up_sweep_function = source_module.get_function(\"prefix_sum_up_sweep\")\n",
    "    blocked_prefix_sum_down_sweep_function = source_module.get_function(\"blocked_prefix_sum_down_sweep\")\n",
    "    blocked_prefix_sum_set_last_block_elements_to_zero_function = source_module.get_function(\n",
    "        \"blocked_prefix_sum_set_last_block_elements_to_zero\")\n",
    "    blocked_prefix_sum_add_block_sums_function = source_module.get_function(\"blocked_prefix_sum_add_block_sums\")\n",
    "\n",
    "    num_sweep_passes = int(math.ceil(math.log(block_size_num_elements, 2)))\n",
    "    block_sums_gpu = np.zeros(block_size_num_elements, dtype=input_data_resized.dtype)\n",
    "\n",
    "    pycuda.driver.memcpy_htod(input_data_resized_device, input_data_resized)\n",
    "    pycuda.driver.memcpy_htod(prefix_sum_device, input_data_resized)\n",
    "    pycuda.driver.memcpy_htod(block_sums_device, block_sums_gpu)\n",
    "\n",
    "    #\n",
    "    # block scan input array\n",
    "    #\n",
    "    prefix_sum_up_sweep_function_block = (block_size_num_threads, 1, 1)\n",
    "    num_blocks = int(math.ceil(float(input_data_resized_num_threads) / float(prefix_sum_up_sweep_function_block[0])))\n",
    "    prefix_sum_up_sweep_function_grid = (num_blocks, 1)\n",
    "\n",
    "    blocked_prefix_sum_set_last_block_elements_to_zero_function_block = (block_size_num_threads, 1, 1)\n",
    "    num_blocks = int(math.ceil(\n",
    "        float(block_size_num_threads) / float(blocked_prefix_sum_set_last_block_elements_to_zero_function_block[0])))\n",
    "    blocked_prefix_sum_set_last_block_elements_to_zero_function_grid = (num_blocks, 1)\n",
    "\n",
    "    blocked_prefix_sum_down_sweep_function_block = (block_size_num_threads, 1, 1)\n",
    "    num_blocks = int(\n",
    "        math.ceil(float(input_data_resized_num_threads) / float(blocked_prefix_sum_down_sweep_function_block[0])))\n",
    "    blocked_prefix_sum_down_sweep_function_grid = (num_blocks, 1)\n",
    "\n",
    "    for d in range(num_sweep_passes):\n",
    "        prefix_sum_up_sweep_function(\n",
    "            prefix_sum_device,\n",
    "            np.int32(input_data_resized_num_elements),\n",
    "            np.int32(d),\n",
    "            block=prefix_sum_up_sweep_function_block,\n",
    "            grid=prefix_sum_up_sweep_function_grid)\n",
    "\n",
    "    blocked_prefix_sum_set_last_block_elements_to_zero_function(\n",
    "        prefix_sum_device,\n",
    "        np.int32(input_data_resized_num_elements),\n",
    "        np.int32(block_size_num_elements),\n",
    "        block=blocked_prefix_sum_set_last_block_elements_to_zero_function_block,\n",
    "        grid=blocked_prefix_sum_set_last_block_elements_to_zero_function_grid)\n",
    "\n",
    "    for d in range(num_sweep_passes - 1, -1, -1):\n",
    "        blocked_prefix_sum_down_sweep_function(\n",
    "            prefix_sum_device,\n",
    "            block_sums_device,\n",
    "            input_data_resized_device,\n",
    "            np.int32(input_data_resized_num_elements),\n",
    "            np.int32(d),\n",
    "            block=blocked_prefix_sum_down_sweep_function_block,\n",
    "            grid=blocked_prefix_sum_down_sweep_function_grid)\n",
    "\n",
    "    #\n",
    "    # block scan block sums array\n",
    "    #\n",
    "    prefix_sum_up_sweep_function_block = (block_size_num_threads, 1, 1)\n",
    "    num_blocks = int(math.ceil(float(block_size_num_threads) / float(prefix_sum_up_sweep_function_block[0])))\n",
    "    prefix_sum_up_sweep_function_grid = (num_blocks, 1)\n",
    "\n",
    "    blocked_prefix_sum_set_last_block_elements_to_zero_function_block = (block_size_num_threads, 1, 1)\n",
    "    num_blocks = int(math.ceil(\n",
    "        float(block_size_num_threads) / float(blocked_prefix_sum_set_last_block_elements_to_zero_function_block[0])))\n",
    "    blocked_prefix_sum_set_last_block_elements_to_zero_function_grid = (num_blocks, 1)\n",
    "\n",
    "    prefix_sum_down_sweep_function_block = (block_size_num_threads, 1, 1)\n",
    "    num_blocks = int(math.ceil(float(block_size_num_threads) / float(prefix_sum_down_sweep_function_block[0])))\n",
    "    prefix_sum_down_sweep_function_grid = (num_blocks, 1)\n",
    "\n",
    "    for d in range(num_sweep_passes):\n",
    "        prefix_sum_up_sweep_function(\n",
    "            block_sums_device,\n",
    "            np.int32(block_size_num_elements),\n",
    "            np.int32(d),\n",
    "            block=prefix_sum_up_sweep_function_block,\n",
    "            grid=prefix_sum_up_sweep_function_grid)\n",
    "\n",
    "    blocked_prefix_sum_set_last_block_elements_to_zero_function(\n",
    "        block_sums_device,\n",
    "        np.int32(block_size_num_elements),\n",
    "        np.int32(block_size_num_elements),\n",
    "        block=blocked_prefix_sum_set_last_block_elements_to_zero_function_block,\n",
    "        grid=blocked_prefix_sum_set_last_block_elements_to_zero_function_grid)\n",
    "\n",
    "    for d in range(num_sweep_passes - 1, -1, -1):\n",
    "        prefix_sum_down_sweep_function(\n",
    "            block_sums_device,\n",
    "            np.int32(block_size_num_elements),\n",
    "            np.int32(d),\n",
    "            block=prefix_sum_down_sweep_function_block,\n",
    "            grid=prefix_sum_down_sweep_function_grid)\n",
    "\n",
    "    #\n",
    "    # distribute scanned block sums back into the prefix sums\n",
    "    #\n",
    "\n",
    "    blocked_prefix_sum_add_block_sums_function_block = (block_size_num_threads, 1, 1)\n",
    "    num_blocks = int(\n",
    "        math.ceil(float(input_data_resized_num_threads) / float(blocked_prefix_sum_add_block_sums_function_block[0])))\n",
    "    blocked_prefix_sum_add_block_sums_function_grid = (num_blocks, 1)\n",
    "\n",
    "    blocked_prefix_sum_add_block_sums_function(\n",
    "        prefix_sum_device,\n",
    "        block_sums_device,\n",
    "        np.int32(input_data_resized_num_elements),\n",
    "        block=blocked_prefix_sum_add_block_sums_function_block,\n",
    "        grid=blocked_prefix_sum_add_block_sums_function_grid)\n",
    "\n",
    "    #\n",
    "    # copy data back to host\n",
    "    #\n",
    "\n",
    "    pycuda.driver.memcpy_dtoh(prefix_sum_gpu, prefix_sum_device)\n",
    "\n",
    "    return prefix_sum_gpu[0:n]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0,  2,  6, 14, 30], dtype=uint32)"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "getPrefixSum_gpu(input_data, n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0,  2,  6, 14, 30], dtype=uint32)"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "getPrefixSum_cpu(input_data, n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "functions = OrderedDict()\n",
    "functions['numpy'] = getPrefixSum_cpu\n",
    "functions['cuda'] = getPrefixSum_gpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The slowest run took 14.84 times longer than the fastest. This could mean that an intermediate result is being cached.\n",
      "100000 loops, best of 3: 4.11 µs per loop\n",
      "1000 loops, best of 3: 1.06 ms per loop\n",
      "100000 loops, best of 3: 6.37 µs per loop\n",
      "1000 loops, best of 3: 1.06 ms per loop\n",
      "100000 loops, best of 3: 8.71 µs per loop\n",
      "1000 loops, best of 3: 1.06 ms per loop\n",
      "100000 loops, best of 3: 11.1 µs per loop\n",
      "1000 loops, best of 3: 1.09 ms per loop\n",
      "100000 loops, best of 3: 13.4 µs per loop\n",
      "1000 loops, best of 3: 1.06 ms per loop\n",
      "100000 loops, best of 3: 15.7 µs per loop\n",
      "1000 loops, best of 3: 1.06 ms per loop\n"
     ]
    }
   ],
   "source": [
    "sizes = [10, 20, 30, 40, 50, 60]\n",
    "\n",
    "scores = pd.DataFrame(data=0, columns=functions.keys(), index=sizes)\n",
    "for size in sizes:\n",
    "    for name, function in functions.items():\n",
    "        data = np.asarray([ 2**i for i in range(1, size+1) ]).astype(np.uint32)\n",
    "        result = %timeit -o function(data, size)\n",
    "        scores.loc[size, name] = result.best"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Results (time in seconds, less is better)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>numpy</th>\n",
       "      <th>cuda</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.000004</td>\n",
       "      <td>0.001062</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.000006</td>\n",
       "      <td>0.001060</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>0.000009</td>\n",
       "      <td>0.001062</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>0.000011</td>\n",
       "      <td>0.001093</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>0.000013</td>\n",
       "      <td>0.001064</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>0.000016</td>\n",
       "      <td>0.001062</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       numpy      cuda\n",
       "10  0.000004  0.001062\n",
       "20  0.000006  0.001060\n",
       "30  0.000009  0.001062\n",
       "40  0.000011  0.001093\n",
       "50  0.000013  0.001064\n",
       "60  0.000016  0.001062"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "hmm, something's gone wrong. \n",
    "But I tried )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
